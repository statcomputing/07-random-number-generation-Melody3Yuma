---
title: "HW7"
author: Xiaoqing Li^[<xiaoqing.li@uconn.edu>; Master student at Department of Mathematics,
  University of Connecticut.]
date: "`r format(Sys.time(), '%d %B %Y')`"
fontsize: 11pt
output:
  pdf_document
---

# Exercise 5.3.1

+ Find the value of the normalizing constant for $g$, i.e., the constant $C$ such that 

\begin{align}
C\int_{0}^{\infty}(2x^{\theta-1} + x^{\theta-1/2})e^{-x}dx = 1
\end{align}

Equation (1)  can be rewritten as 
\begin{align}
2C\int_{0}^{\infty}(x^{\theta-1}e^{-x})dx + C\int_{0}^{\infty}(\sqrt x x^{\theta-1}e^{-x})dx = 1
\end{align}

The first term is an integral of a gamma function with parameter $(\theta, 1)$ and the second term is the $E(\sqrt(X))$ where $X \sim Gamma(\theta, 1)$. Therefore we have 

\begin{align}
2C\Gamma(\theta) + 1/2C\Gamma(\theta) = 1
\end{align}

Then we have $C=\frac{2}{5\Gamma(\theta)}$. So the weights are $2C = \frac{4}{5\Gamma(\theta)}$ and $1/2C=\frac{1}{5\Gamma(\theta)}$. The first component follows a $Gamma(\theta, 1)$ distribution, the second term is actually a Nakagami distribution, which is the square root of $Gamma(\theta, 1)$.

+ Design a procedure (pseudo-code) to sample from $g$, implement it in an R function, draw a sample of size $n=10,000$ using your function for at least one $\theta$ value, plot the kernel density estimation of $g$ from your sample and the true density in one figure. 

The CDF of $g(x)$ can be obtained by adding the CDF of two gamma distributions. So we have

\begin{align}
G(x) = \frac{4}{5}\gamma(\theta,x) + \frac{1}{5}\frac{\gamma(\theta, x^2)}{\Gamma(\theta)}
\end{align}

where $\gamma()$ and $\Gamma()$ are incomplete and complete gamma functions. 

```{r, message=FALSE, warning=FALSE}
library('expint')

MixGammaCDF <- function(x, u){
  theta <- 2
  0.8*gammainc(theta, x) + 0.2*gammainc(theta, x^2)/gamma(theta) - u
}
set.seed(1021)
n <- 10000
u <- runif(n)
x.vec <- NULL

for(i in 1:n){
  x <- uniroot(MixGammaCDF, c(0, 10e10), tol = 0.0001, u=u[i])$root
  x.vec <- c(x.vec, x)
}

den <- density(x.vec)
xfit <- den$x[which(den$x > 0)]
yfit <- den$y[which(den$x > 0)]

gx <- function(x, theta){
  (2*x^(theta-1) + x^(theta-0.5))*exp(-x)*0.4/gamma(theta)
}

TrueY <- gx(xfit, theta=2)

#plot(xfit,TrueY, ylim=c(0, max(c(TrueY, yfit))), type = 'l', xlab='x', ylab='g(x)', main='Red=Sample Kernel and black=Thereotical Kernel')
#lines(xfit, yfit, col = 'red')  ## from sample

hist(x.vec, freq = FALSE, breaks=40,ylim = c(0,0.5),
     xlab='x', ylab='g(x)', main='Red=Sample Kernel and black=Thereotical Kernel')
lines(xfit, TrueY)
lines(xfit, yfit, col='red')
```

+ Design a procedure (pseudo-code) to use rejection sampling to sample from $f$ using $g$ as the instrumental distribution. Overlay the estimated kernel density of a random sample generated by your procedure and $f$.

It is easy to verify that $f(x) < g(x)$. So we can let $M = 1$ for the rejection method.

```{r}

fx <- function(x, theta=2){
  sqrt(4+x)*x^(theta-1)*exp(-x)
}


Const = integrate(fx, lower=0, upper=Inf)

fx2 <- function(x, theta=2){
  sqrt(4+x)*x^(theta-1)*exp(-x)/2.434154
}

gx2 <- function(x, theta=2){
  (2*x^(theta-1) + x^(theta-0.5))*exp(-x)*0.4/gamma(theta)
}

u1 <- runif(n)
index.vec <- NULL
for(i in 1:n){
  index <- ifelse(u1[i] <= fx2(x.vec[i])/gx2(x.vec[i]), TRUE, FALSE)
  index.vec <- c(index.vec, index)
}

x.vec1 <- x.vec[index.vec]

den <- density(x.vec1)
xfit1 <- den$x[which(den$x > 0)]
yfit1 <- den$y[which(den$x > 0)]

TrueY <- fx2(xfit1, theta=2)

hist(x.vec1, freq = FALSE, breaks=40,ylim = c(0, 0.5),
     xlab='x', ylab='f(x)', main='Red=Sample Kernel and black=Thereotical Kernel')
lines(xfit1, TrueY)
lines(xfit1, yfit1, col='red')

```

# Exercise 6.3.1

```{r, warning=FALSE, message=FALSE, fig.width=6, fig.height=4}
library(HI)

delta <- 0.7 # true value to be estimated based on the data
n <- 1000
set.seed(123)


x.vec <- NULL
for(i in 1:n){
  u <- rbinom(1, prob = delta, size = 1)
  mu1 <- rnorm(1, 0, 10)
  mu2 <- rnorm(1, 0, 10)
  sigma1 <- sqrt(1/rgamma(1, shape = 0.5, scale = 10))
  sigma2 <- sqrt(1/rgamma(1, shape = 0.5, scale = 10))
  x1 <- rnorm(1, mu1, sigma1)
  x2 <- rnorm(1, mu2, sigma2)
  x <- ifelse(u == 1, x1, x2)
  x.vec <- c(x.vec, x)
}




logpost <- function(theta, x) {
  delta <- theta[1]
  mu1 <- theta[2]
  mu2 <- theta[3]
  sigma1 <- theta[4]
  sigma2 <- theta[5]
  sum(log(delta * dnorm(x, mu1, sigma1) + (1 - delta) * dnorm(x, mu2, sigma2)))
}


mymcmc <- function(niter, thetaInit, data, nburn= 100) {
  p <- length(thetaInit)
  thetaCurrent <- thetaInit
  ## define a function for full conditional sampling  
  logFC <- function(th, idx) {
    theta <- thetaCurrent
    theta[idx] <- th
    logpost(theta, data)
  }
  out <- matrix(thetaInit, niter, p, byrow = TRUE)
  ## Gibbs sampling
  for (i in 2:niter) {
    for (j in 1:p) {
      ## general-purpose arms algorithm
      out[i, j] <- thetaCurrent[j] <-
        HI::arms(thetaCurrent[j], logFC,
                 function(x, idx) ((min(x) > -50) * (max(x) < 50)), 
                 1, idx = j)
    }
  }
  out[-(1:nburn), ]
}


niter <- 1000; nburn <- 200
thetaInit <- c(0.7, 0, 0, 10, 10)
sim <- mymcmc(niter, thetaInit, x.vec)
plot(ts(sim[,1]))
plot(ts(sim[,2]))
plot(ts(sim[,3]))
plot(ts(sim[,4]))
plot(ts(sim[,5]))


```